[{"content":"Overview Kubernetes에서 gRPC를 사용하는 MSA기반의 솔루션이 있는데, 로드 밸런싱이 안 된다는 문제를 전달 받았다.\n자세한 내용을 보기 전에, 상황에 맞는 해결 방법을 고민해보자.\n다시 언급하겠지만, 해결 방법은 3가지가 있다.\nClientSide Load Balancing L7 Load Balancing Service Mesh 1번 Client Side\n프록시로 인한 레이턴시 없음 추가적인 리소스(Deployment \u0026hellip;)가 필요 없음 client에서 round_robin 로드밸런싱 설정 및 K8s Headless Service 변경 필요 server에서 적절한 \u0026lsquo;MaxConnectionAge\u0026rsquo;을 찾아서 설정이 필요 2번 L7 LB\nL7 로드밸런서(여기선 traefik)을 선정해서 구성해야 하며, 관리 포인트가 생김 API Gateway 패턴 구조이면 괜찮은 방안이 될 수 있지만, 그 외에는 다른 방법이 더 좋은 듯 함. 3번 Service mesh\n구축을 하더라도 우리 조직과 고객사에서 안정적으로 관리하기 위한 인력이 있는가? 서비스 규모가 꽤 크고 충분한 인력이 있다면 다른 기능도 활용가능한 좋은 방안이지만, 그렇지 않다면 관리면에서 사이드 이펙트가 큼. Tricky Load Balancing Client가 gRPC 서버로 Connection을 맺은 후, 서버의 cpu/memory 사용량이 증가하면서 auto scaling policy으로 새 pod가 생성된다. 그러나, 새 pod에 트래픽이 전달되지 않는 것을 볼 수 있는데, 이는 gRPC 글에서 얘기한 Multiplexing으로 인한 것이다.\n이를 해결하기 위한 방법으로 크게 3가지 방법이 있다. 상황에 맞는 방식을 선택하는게 필요하며, 이번에는 1, 2번을 적용해보고 다음에 Service Mesh를 적용해보자. (성능은 proxyless가 가장 좋다)\nClient-side Load Balancing L7 Load Balancing Service Mesh proxy proxyless (xds) (https://kubernetes.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears/)\nClient-side Load Balancing gRPC는 Client-side Load Balancing 기능을 제공하는데,pick_fist, round_robin 2가지 방식이 있다.\nDefault는 pick_first인데, 말 그대로 resolver에서 주소 리스트를 받아 첫 번째 주소로 연결을 시도하고, RPC들은 Channel을 통해 보내진다. 연결이 끊길 경우, backkoff로 다시 주소 리스트에서 연결 가능한 곳으로 connection을 맺게 된다.\nround_robin은 resolver로부터 주소 리스트를 받아서, 각 주소에 대해 subchannel을 생성하고, 연결이 끊기면 다시 연결을 시도한다.\n(https://github.com/grpc/grpc/blob/master/doc/load-balancing.md#load-balancing-policies)\n아래에서 pick first, round robin 의 예시를 볼 수 있다.\n(https://github.com/grpc/grpc-go/blob/master/examples/features/load_balancing/README.md)\nthis is pick first (from :10001) this is pick first (from :10001) this is pick first (from :10001) ... this is round robin (from :10001) this is round robin (from :10002) this is round robin (from :10003) ... 위의 예시를 테스트해보면 Round-robin 설정으로 여러 서버에 로드밸런싱이 되는 것을 확인할 수 있다. 그러나 Kubenetes에서 추가로 필요한 작업들이 몇 가지 있다.\n위의 예제에서는 서버 IP를 가지고 Custom resolver을 사용했지만, 나는 Kubernetes Service 도메인으로부터 Pods의 IP 주소들을 가져오려고 한다. gRPC의 기본 resolver를 사용할 경우, Default는 passthrough방식인데, 이는 Client가 Service 이름에서 여러 IP 주소를 얻을 수 없기에, dns scheme를 기본으로 설정해줘야 한다.\nhttps://pkg.go.dev/google.golang.org/grpc/resolver#SetDefaultScheme https://github.com/grpc/grpc-go/blob/master/resolver/resolver.go#L39 그럼, 계속 이어서 다음으로 필요한 내용들을 보자.\nKuberntes의 Service 레이어는 Client 요청을 L4 레벨 로드밸런싱으로 Service에 연결된 Pods 중 하나로 전달해준다.\nClient에서 Lound-robin방식으로 설정한다 해도 이 서비스 레이어의 L4, persistent connection으로 인해, 한 Client의 요청은 하나의 Pod로만 전달되고, 다른 replica pods들은 트래픽을 받지 못하게 된다.\n(https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/)\n그럼, 어떻게 하면 될지 계속 살펴보자.\nKubernetes는 Headless Service로 Service에 대한 단일 IP가 아닌, 각 포드 IP에 대한 다중 A 레코드를 생성할 수 있는데, 이를 통해 Client가 직접 replica Pods에 대한 주소 리스트를 받아서 로드밸런싱을 할 수 있다.\n그런데 \u0026hellip; 여기서 문제가 하나 더 있다. autoScaling 설정으로 사용자 트래픽이 증가하면서 생성된 새 Pods에는 요청이 전달되지 않는 것이다. 왜냐하면, Client가 Resolver로부터 주소 리스트를 받아서 subchannels를 생성한 후에, 서버로 연결이 끊어져 실패하기 전까지 새로운 주소로 연결을 시도하지 않기 때문이다. 그래서, MaxConnectionAge를 설정하여 일정시간이 지나면 재연결을 하는 방법이 있는데, 설정 값에 따라 pod가 새로 생성된 후 트래픽이 전달되기까지는 시간이 걸릴 것이다.\n이제, 위 내용을 토대로 Kubernetes에서 Client Load Balancing 예시를 보겠다.\nClient\nresolver.SetDefaultScheme(\u0026#34;dns\u0026#34;) conn, err := grpc.Dial( \u0026#34;\u0026lt;service-name\u0026gt;\u0026lt;namespace\u0026gt;.svc.cluster.local\u0026#34;, grpc.WithDefaultServiceConfig(`{\u0026#34;loadBalancingConfig\u0026#34;: [{\u0026#34;round_robin\u0026#34;:{}}]}`), ... ) Server\nserver := grpc.NewServer( grpc.KeepaliveParams(keepalive.ServerParameters{MaxConnectionAge: 1 * time.Minute, MaxConnectionAgeGrace: 30 * time.Second}), ... ) Kubernetes\nkind: Service spec: clusterIP: None ... L7 Load Balancer gRPC는 TCP connection을 유지하기에, Kubernetes Service의 L4가 아닌 L7 Load Balancer가 따로 필요하다.\nL7 로드밸런싱을 제공하는 여러 Load Balancer가 있는데, 그 중 Traefik으로 진행해본다.\n(https://kubernetes.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears/)\n서비스에 대한 manifest 파일을 클러스터에 적용하고, 오토스케일링을 위해 metrics-server도 배포해준다.\n그 다음, Traefik도 알맞게 설정 후 적용한다.\n아래에서는, test-a, test-b, test-c gRPC 서버들이 있고, client에서 요청을 전달한다고 가정한다.\nkubectl apply -f metrics-server.yaml kubectl apply -f test-a.yaml -f test-b.yaml -f test-c.yaml helm repo add traefik https://helm.traefik.io/traefik helm repo update helm search repo traefik/traefik --versions helm show values traefik/traefik --version \u0026lt;version\u0026gt; traefik-values.yaml # traefik-values 파일을 수정해서, 설정 오버라이딩 helm install traefik-l7-lb traefik/traefik --version \u0026lt;version\u0026gt; -n dongle -f traefik-values-override.yaml kubectl apply -f traefik-route.yaml test-a\nkind: Service metadata: name: test-a-service namespace: dongle spec: type: ClusterIP selector: app.kubernetes.io/name: test-a ports: - protocol: TCP: port: 10001 targetPort: 10001 ... --- kind: Deployment metadata: name: test-a namespace: dongle spec: replicas: 1 ... --- kind: HorizontalPodAutoscaler metadata: name: test-a-hpa namespace: dongle spec: scaleTargetRef: kind: Deployment name: test-a minReplicas: 1 maxReplicas: 3 ... traefik-values-override\n# https://github.dev/traefik/traefik-helm-chart/blob/master/traefik/templates/service.yaml # https://github.com/traefik/traefik-helm-chart/blob/master/traefik/VALUES.md#values # https://github.com/traefik/traefik-helm-chart/blob/master/traefik/VALUES.md ports: test-a: port: 10001 expose: true exposedPort: 10001 protocol: TCP test-b: port: 10002 expose: true exposedPort: 10002 protocol: TCP test-c: port: 10003 expose: true exposedPort: 10003 protocol: TCP service: type: LoadBalancer traefik-route\n# https://doc.traefik.io/traefik/routing/providers/kubernetes-crd/#kind-ingressroute # https://doc.traefik.io/traefik/user-guides/grpc/ apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: test-a-ingress-route namespace: dongle spec: entryPoints: - test-a routes: - match: PathPrefix(`/`) kind: Rule services: - name: \u0026#34;test-a-service\u0026#34; port: 10001 scheme: h2c --- apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: test-b-ingress-route namespace: dongle spec: entryPoints: - test-b routes: - match: PathPrefix(`/`) kind: Rule services: - name: \u0026#34;test-b-service\u0026#34; port: 10002 scheme: h2c --- apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: test-c-ingress-route namespace: dongle spec: entryPoints: - test-c routes: - match: PathPrefix(`/`) kind: Rule services: - name: \u0026#34;test-b-service\u0026#34; port: 10003 scheme: h2c ","date":"2023-08-23T19:44:57+09:00","image":"https://kimsehyoung.github.io/post/grpc/grpc_load_balancing/load-balancing_hu9853b85549f8f6a0aa4a260c374db0ce_55651_120x120_fill_box_smart1_3.png","permalink":"https://kimsehyoung.github.io/post/grpc/grpc_load_balancing/","title":"gRPC Load Balancing in Kubernetes"},{"content":"Overview Google 내부에서 마이크로 서비스를 연결하는데 사용한 Stubby라는 RPC 인프라를 표준화한 오픈소스 프레임워크\n가장 큰 특징은 Probocol Buffers, HTTP/2를 사용\nProtobuf는 google에서 만든 구조화된 데이터를 직렬화하는 방식으로 JSON 형식보다 작은 크기로 효율적 HTTP/2는 헤더 압축, 이진 형식, 서버 푸시, 멀티플렉싱\u0026hellip; 으로 성능 향상을 가져옴 // Protobuf로 서비스 API를 정의 message HelloRequest { string msg = 1; } message HelloResponse { string msg = 1; } service HelloService { rpc SayHello (HelloRequest) returns (HelloResponse); } RPC RPC(Remote Procedure Call)는 클라이언트가 서버의 procedure(함수)를 마치 로컬에 있는 것처럼 호출하는 것이다. 다음 동작 단계를 보면서, 어떤 건지 알아보자.\nRPC는 로컬 메서드를 호출하는 것처럼 보이지만, 클라이언트와 서버는 별도의 프로세스로 다른 주소 공간을 가지므로, 서로 통신을 위해서 JSON, Ptotobuf, Thrift 같은 IDL(Interface Definition Language)로 호출 규약을 정의한다. rpcgen으로 IDL 파일을 컴파일하여 클라이언트와 서버의 Stub 코드를 생성한다. Stub은 Source code(C++, Java, Python, Go\u0026hellip;)의 형태로 서버에서는 Stub을 사용하여 프로시저에 대한 기능 구현이 필요하다. 각 클라이언트, 서버에서는 Stub 코드를 같이 빌드한다. 클라이언트는 전달할 매개변수를 Mashalling하고, 로컬 Stub 프로시저를 호출하여 서버에 메시지를 전송한다. 서버는 Stub을 통해 해당 프로시저가 호출되고, 매개변수를 Unmarshalling하여 요청을 수행한 결과를 Mashalling하여 클라이언트에게 전송한다. 클라이언트는 해당 서버 프로시저에 대한 결과를 받으며, 마치 로컬 메서드를 호출하는 것처럼 사용을 할 수 있게 된다. (https://learn.microsoft.com/en-us/windows/win32/rpc/how-rpc-works)\nHTTP/2 Multiplexing 위에서 언급한 듯이 gRPC에서 주요 특징 중 하나는 HTTP/2를 사용하는 것이다.\n그중에 다음 글의 Load Balancing에서 중요한 Multiplexing에 대해 조금 살펴보자.\nHTTP/1.0에서 매 요청마다 연결하고 닫는 오버헤드가 있었고, HTTP/1.x에서 TCP connection 재사용으로 네트워크 효율을 개선하고자 persistent connection(aka long-lived)이 도입되었다.\n그러나, 한 번에 하나의 요청과 응답만을 처리할 수 있었고, 이는 Head-of-Line blocking(앞선 요청이 오래걸릴 경우, 다음 요청들이 지연)을 야기했다.\nHTTP/2에서는 Multiplexing으로 단일 TCP 연결(persistence connection)에서 여러 요청과 응답을 순서와 상관없이 동시에 처리하여, HOL blocking 문제를 해결하였다.\n(https://www.imperva.com/learn/performance/http2/)\n그럼, HTTP/2에 대해 조금 더 알아보자.\nHTTP/2에서 통신의 최소 단위는 Binary 형태의 Frame으로, 하나의 HTTP Message를 여러 개의 Frames으로 나누어 보내는데, 연결된 Connection에서 양방향 데이터 흐름인 Stream으로 Frames을 전송하고 목적지에서 재조립하게 된다.\n(☞ HTTP/1.x의 통신 단위는 Plain text 형태인 요청/응답 Message)\n키 포인트인 Multiplexing은 Single Connection에서 동시에 여러 Streams을 처리하는 것이다.\n또한, 각 요청/응답 메시지에서 분할된 Frame들은 동일한 Stream에서 전송되며 스트림 식별자를 가지고 있어서, 수신 측에서 프레임들이 동일한 메시지의 일부임을 알 수 있게 된다.\n(https://www.cncf.io/blog/2018/07/03/http-2-smarter-at-scale/)\ngRPC Channel gRPC에서 새로운 개념인 channel이 있는데, HTTP/2와 함께 알아보자.\nChannel은 특정 Host와 Port의 gRPC 서버에 대한 Connection을 추상화한 인터페이스이며, 이를 통해 연결을 유지하고 관리하며, 효과적으로 활용할 수 있게 해준다.\n또한, HTTP/2는 single connection에서 동시에 여러 메시지 스트림을 전송할 수 있게 해줬다면, channel은 여기서 좀 더 확장되어, 동시에 multiple connections에서 여러 메시지 스트림을 가능하게 한다.\n// Create a gRPC channel in Go conn, err := grpc.Dial(*serverAddr, opts...) (https://grpc.io/blog/grpc-on-http2/)\ngRPC-Gateway gRPC는 좋은 장점들이 있지만, 몇 가지 고려할 점이 있다.\n개발자들이 gRPC에 익숙지 않거나, 기존 서비스들이 REST 기반이라면 한 번에 서비스 전체에 gRPC를 적용하기 어려움 브라우저에서 gRPC가 지원이 안 되기에, 추가적으로 gRPC-Web 또는 gRPC-Gateway 필요 이러한 이유들로 gRPC-Gateway를 사용할 수 있다.\ngRPC Gateway는 REST를 gRPC로 변환하는 리버스 프록시 서버를 생성하는 방식으로 gRPC, REST 모두 호출 가능하게 해준다.\n(https://github.com/grpc-ecosystem/grpc-gateway)\nPros and Cons Pros\nPolyglot 환경 (protobuf를 기반으로 gRPC가 지원하는 다양한 언어로 Stub코드 생성) 양방향 통신 (Unary, Client streaming, server streaming, Bidirectional Streaming) 네트워크 성능 (Protobuf, HTTP/2) API 문서 없이 Protobuf만으로 인터페이스 명세 (필요 시, gRPC-Gateway plugin으로 OpenAPI 문서 생성 가능) Interceptor, Load balancing 같은 유용한 내장 기능 제공 Cons\nJSON/REST를 주로 사용했다면, 러닝 커브로 인한 도입 어려움 gRPC는 브라우저 직접적인 지원이 안 되기에, 추가적인 방법이 필요 TCP 연결을 유지하는 HTTP/2의 특성으로 인해, 까다로운 로드 밸런싱 References https://www.cncf.io/blog/2018/07/03/http-2-smarter-at-scal https://grpc.io/docs/languages/go/basics/ ","date":"2023-08-23T19:11:30+09:00","image":"https://kimsehyoung.github.io/post/grpc/grpc/grpc_hu6f115954a23a43b81942d129ea8277b1_80687_120x120_fill_box_smart1_3.png","permalink":"https://kimsehyoung.github.io/post/grpc/grpc/","title":"gRPC를 알아보자"},{"content":" prerequisite container runtime Use containerd (containerd, CRI-O, docker) sudo apt-get update sudo apt-get install -y \\\\ ca-certificates \\\\ curl \\\\ gnupg \\\\ lsb-release sudo mkdir -p /etc/apt/keyrings curl -fsSL \u0026lt;https://download.docker.com/linux/ubuntu/gpg\u0026gt; | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg echo \\\\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] \u0026lt;https://download.docker.com/linux/ubuntu\u0026gt; \\\\ $(lsb_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get update sudo apt-get install containerd.io=1.6.14-1 sudo apt-mark hold containerd.io # Disabled cri plugin such as below line in config.toml after installing containerd package # disabled_plugins = [\u0026#34;cri\u0026#34;] # So, set the config to default # Use SystemdCgroup and restart daemon containerd config default | sudo tee /etc/containerd/config.toml sudo sed -i \u0026#39;s/SystemdCgroup = false/SystemdCgroup = true/\u0026#39; /etc/containerd/config.toml sudo systemctl restart containerd network overlay enables networking between nods br_netfilter enables networking between pods cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # sysctl params required by setup, params persist across reboots cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF # Apply sysctl params without reboot sudo sysctl --system swap off swap memory support from v 1.22 # Check swap memory \u0026#34;swapon -s\u0026#34; or \u0026#34;free -h\u0026#34; # Disable swap sudo swapoff -a # Disable swap on startup sudo sed -i \u0026#39;/ swap / s/^\\\\(.*\\\\)$/#\\\\1/g\u0026#39; /etc/fstab # Implementation after reboot (crontab -l 2\u0026gt;/dev/null; echo \u0026#34;@reboot /sbin/swapoff -a\u0026#34;) | crontab - || true setup cluster caution pod\u0026rsquo;s CIDR block overlap control-plane node TBD: use public/private IP for api server address\npod-cidr: set according to your environment\ne.g. Use 10.244.0.0/16 host: 192\u0026hellip;, docker: 172\u0026hellip;\n# Install packages needed to use the Kubernetes apt repository sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl # Download the Google Cloud public signing key: sudo curl -fsSLo /etc/apt/keyrings/kubernetes-archive-keyring.gpg \u0026lt;https://packages.cloud.google.com/apt/doc/apt-key.gpg\u0026gt; # Add the Kubernetes apt repository echo \u0026#34;deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] \u0026lt;https://apt.kubernetes.io/\u0026gt; kubernetes-xenial main\u0026#34; | sudo tee /etc/apt/sources.list.d/kubernetes.list # Install sudo apt-get update apt-cache madison kubeadm | head -20 sudo apt-get install -y kubelet=1.26.0-00 kubeadm=1.26.0-00 kubectl=1.26.0-00 sudo apt-mark hold kubelet kubeadm kubectl # Check packages kubelet --version kubeadm version kubectl version --client # Pre-pull the required control-plane images kubeadm config images list --kubernetes-version=v1.26.0 sudo kubeadm config images pull --kubernetes-version=v1.26.0 # Set up the Kubernetes control plane sudo kubeadm init --apiserver-advertise-address=\u0026lt;private-ip\u0026gt; --apiserver-cert-extra-sans=\u0026lt;private-ip\u0026gt; --pod-network-cidr=\u0026lt;pod-cidr\u0026gt; --node-name $(hostname -s) mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config kubectl get nodes kubectl get namespaces kubectl get pods -n kube-system network plugins Use calico cni flannel: \u0026lsquo;10.244.0.0/16\u0026rsquo;, calico: \u0026lsquo;192.168.0.0/16\u0026rsquo; … To avoid overlapping, modify cidr to \u0026lsquo;10.244.0.0/16\u0026rsquo; curl \u0026lt;https://raw.githubusercontent.com/projectcalico/calico/v3.24.5/manifests/calico.yaml\u0026gt; -O # Edit \u0026#39;CALICO_IPV4POOL_CIDR\u0026#39; in calico.yml, if CIDR block is changed from default for avoiding overlap. kubectl apply -f calico.yaml kubectl describe node | egrep \u0026#39;^Name|PodCIDR\u0026#39; helm install curl -fsSL -o get_helm.sh \u0026lt;https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\u0026gt; chmod 700 get_helm.sh ./get_helm.sh helm version reference Read references while following below guide step by step\nset-up https://kubernetes.io/docs/setup/production-environment/container-runtimes/ https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ runtime https://kubernetes.io/ko/docs/setup/production-environment/container-runtimes/https://github.com/containerd/containerd/blob/main/docs/getting-started.md network https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-networkhttps://projectcalico.docs.tigera.io/getting-started/kubernetes/self-managed-onprem/onpremises#install-calico-with-kubernetes-api-datastore-more-than-50-nodes swap memory https://kubernetes.io/ko/docs/concepts/architecture/nodes/#swap-memory port https://kubernetes.io/docs/reference/networking/ports-and-protocols/ helm https://helm.sh/docs/intro/install/ ","date":"2023-08-13T16:30:14+09:00","image":"https://kimsehyoung.github.io/post/kubernetes/kubeadm/kubeadm_hu52b047000e5acb39470b30f94d1ddcf0_25140_120x120_fill_box_smart1_3.png","permalink":"https://kimsehyoung.github.io/post/kubernetes/kubeadm/","title":"Kubernetes 설치"},{"content":"Trunk based 브랜치 관리가 간단하고, CI/CD 와 함께 신속, 지속적인 개발과 배포에 용이 코드베이스가 항상 배포 가능한 상태를 유지하므로, 긴급 수정이나 기능 추가가 빠르게 이루어짐. 모든 개발자가 단일 브랜치에서 작업하기 때문에, 코드 품질과 안정성 관리에 주의가 필요함. Branch\nmain short-term Git flow 2010 프로젝트의 안전성과 관리를 높이는데 초점을 맞춤. 브랜치 관리가 복잡하고, 신속한 배포가 어려움. Branch\nmain 항상 production에 배포 가능한 안정적인 코드를 유지하며, 모든 작업의 기준 브랜치 (permanent) develop 개발자가 feature 브랜치를 merge (permanent) feature 기능 개발을 위한 브랜치로, 개발이 완료되면 develop 브랜치로 merge release 배포 준비를 위해 QA(Intergration Test) 및 bug fix 후 main, develop로 merge hotfix 긴급한 버그 수정을 위한 브랜치로, main에서 분기하여 수정 후 main, develop으로 merge GitHub flow 2011 git flow 의 복잡성을 줄이기 위해 고안됨. 빠른 개발 및 배포와 피드백 Branch\nmain production에 배포되는 안정적인 버전의 브랜치이며, 좀 더 엄격한 규칙이 필요(permanent) feature 기능 개발을 위한 브랜치로, 개발이 완료되면 main 브랜치로 merge 브랜치명은 작업 내용을 표현할 수 있도록 명명 GitLab flow 2014 Git flow와 GitHub flow의 중간 정도의 혼합 방식 환경 별 브랜치 사용 Branch\nmain 항상 production에 배포 가능한 안정적인 코드를 유지하며, 모든 작업의 기준 브랜치 (permanent) feature 기능 개발이 완료되면 main 브랜치로 merge staging(optional) staging환경에서 변경 사항을 테스트 및 검증하여 문제가 없으면 production으로 병함 (permanent) production 실제 서비스에 배포되는 브랜치 (permanent) 어떻게 적용하면 좋을까? 복잡하고 큰 규모의 프로젝트, 정기 릴리스가 필요한 때 Git Flow를 적용하면 좋을 것 같다. 차량, IOT 기기와 같은 분야에서 많이 이용될 것이고, 참여했던 차량 SW 프로젝트에서 기반으로 한 것이 Git Flow이다.\n빠른 변화와 피드백, 하루에도 여러 번의 배포가 이루어진다면 Trunk Based, GitHub Flow 기반의 Workflow를 사용할 수 있다. SaaS, Agile 방식에 적합하고, 단순하지만 그만큼 자동화 테스트, 롤백, 코드 리뷰와 같은 개발 문화 등이 이루어져야 할 것이다.\nWorkflow는 딱 정하는 것이 아닌 조직, 프로젝트, 여러 가지 현재 상황에 맞추어 가는 것이 필요하다. 실제로 안정성을 위해 \u0026lsquo;feature-develop-main\u0026rsquo;, \u0026lsquo;feature-main-production\u0026rsquo;로 구성을 했다가, CI/CD에 unit test, e2e를 적용 후에 \u0026lsquo;feature-main\u0026rsquo;으로 변경을 했었다.\n","date":"2023-08-13T10:15:29+09:00","image":"https://kimsehyoung.github.io/post/git/workflow/git_workflow_hu3d03a01dcc18bc5be0e67db3d8d209a6_2538919_120x120_fill_q75_box_smart1.jpg","permalink":"https://kimsehyoung.github.io/post/git/workflow/","title":"Git Workflow"},{"content":"Network tcpdump tcpdump prints out a description of the contents of packets on a network interface. https://www.tcpdump.org/manpages/tcpdump.1.html\ntcpdump dst 192.168.11.11 and port 22 tcpdump -i eno1 host 192.168.11.11 tcpdump host 192.168.11.11 and tcp port 443 -w tcpdump.log tcpdump -r tcpdump.log nc nc scans TCP and UDP connections\nhttps://linux.die.net/man/1/nc\nnc -zv 192.168.123.123 443 =\u0026gt; Connection to 192.168.123.123 443 port [tcp/*] succeeded! arping arping sends ARP request to a neighbor host.\nhttps://man7.org/linux/man-pages/man8/arping.8.html\nsudo arping 192.168.123.123 =\u0026gt; 60 bytes from a1:b2:c3:d4:e5:f6 (192.168.123.123) ... ETC top top displays system info as well as a list of processes or threads currently managed by the Linux Kernel.\nhttps://man7.org/linux/man-pages/man1/top.1.html\ntop -p \u0026lt;pid\u0026gt; -H tee tee reads from standard input and writes to standard output and files.\nhttps://man7.org/linux/man-pages/man1/tee.1.html\necho \u0026#34;hello\u0026#34; | tee OUTFILE cat OUTFILE | tee NEWFILE echo \u0026#34;hello\u0026#34; | tee -a OUTFILE echo \u0026#34;hello\u0026#34; | tee -a OUTFILE /dev/null Why use tee?\n-\u0026gt; It might be thought that echo, cat is sufficient. But, They can’t write into root files.\n# \u0026#34;permission denied\u0026#34; occured. sudo echo \u0026#34;Hello\u0026#34; \u0026gt;\u0026gt; /root/test.txt # It works successfully. echo \u0026#34;Hello\u0026#34; | sudo tee -a /root/test.txt lsof lsof lists file information about files opened by processes.\nhttps://man7.org/linux/man-pages/man8/lsof.8.html\nsudo lsof -a -p 1234567 -d cwd COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME python3 1234567 test cwd DIR 8,2 4096 5656565 /home/test/test-server screen screen multiplexes a physical terminal to each virtual terminal.\nhttps://linux.die.net/man/1/screen\nWhen a long-running task on a remote machine is performed, It helps that the SSH session is terminated and the work is corrupted or lost.\n# Create a new session screen -S test # Detach the session ctrl-a + d screen -ls # Resume a detached screen session. screen -r test # Exit with the session termination. exit pstree pstree displays a tree of process.\nhttps://man7.org/linux/man-pages/man1/pstree.1.html\nscreen -S test ./test-app \u0026amp; ctrl-a + d screen -ls pstree -p 1234567 lsblk lsblk lists information about all available or the specified block devices.\nhttps://man7.org/linux/man-pages/man8/lsblk.8.html\nlsblk -o name,rota,size,mountpoint mount mount serves to attach the file system on some device to the specific directory.\nhttps://linux.die.net/man/8/mount\nmount /dev/sda /backup unmount /dev/sda df -h ","date":"2023-08-12T23:43:26+09:00","image":"https://kimsehyoung.github.io/post/linux/commands/linux_commands_hu3d03a01dcc18bc5be0e67db3d8d209a6_1006554_120x120_fill_q75_box_smart1.jpg","permalink":"https://kimsehyoung.github.io/post/linux/commands/","title":"리눅스 명령어"}]